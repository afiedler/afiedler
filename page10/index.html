<!DOCTYPE html>
<html lang="en-us">

  <head>
  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>
    
      Andy Fiedler &middot; 
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/public/css/poole.css">
  <link rel="stylesheet" href="/public/css/syntax.css">
  <link rel="stylesheet" href="/public/css/hyde.css">
  <link rel="stylesheet" href="http://fonts.googleapis.com/css?family=PT+Sans:400,400italic,700|Abril+Fatface">
  <link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css" rel="stylesheet" integrity="sha256-3dkvEK0WLHRJ7/Csr0BZjAWxERc5WH7bdeUya2aXxdU= sha512-+L4yy6FRcDGbXJ9mPG8MT/3UCDzwR9gPeyFNMCtInsol++5m3bk2bXWKdZjvybmohrAsn3Ua5x8gfLnbE1YkOg==" crossorigin="anonymous">

  <!-- Icons -->

  <!-- RSS -->
  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <!-- JS -->
  <script>
    // Picture element HTML5 shiv
    document.createElement( "picture" );
  </script>
  <script async src="/public/js/picturefill.min.js"></script>
</head>


  <body>

    <div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <h1>
        <a href="/">
          Andy Fiedler
        </a>
      </h1>
      <p class="lead"></p>
    </div>

    <nav class="sidebar-nav">
      <a class="sidebar-nav-item" href="/">Home</a>

      

      
      
        
          
        
      
        
          
        
      
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
            
              <a class="sidebar-nav-item" href="/projects/">Projects</a>
            
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      
        
          
        
      

    </nav>

    <div class="sidebar-footer">
      <p>&copy; 2016 <a href="https://creativecommons.org/licenses/by-nc/4.0/">CC-BY-NC</a>.</p>
    </div>
  </div>
</div>


    <div class="content container">
      <div class="posts">
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2014/05/the-ultimate-guide-to-dealing-with-high-frequency-data">
        The Ultimate Guide to Dealing with High Frequency Data
      </a>
    </h1>

    <span class="post-date">07 May 2014</span>

    <p>I have no idea if this is actually the ultimate guide to high frequency data, but hopefully it is at least a useful guide!</p>

<p>I’m currently working on a project to replace the <a title="Time Series Database" href="http://andyfiedler.com/projects/time-series-database/">Time Series Database</a> (TSDB) that I wrote a few years ago. By re-writing it, I’m learning a lot about what works and what doesn’t when dealing with high frequency data. High frequency in this case means billions of records, with timestamp precision down to the millisecond. This data is being used for economic research and analytics, not live trading. It is a pain to deal with because it is simply too large to fit in memory or process with a standard relational database. These are some rules that I’ve developed while working on this project.</p>
<h2>Partition your data!</h2>
<p>The biggest issue with this much time series data is that you simply cannot index the timestamp column with any normal indexing scheme. In DB-speak, the timestamp column will be a <a title="Cardinality (Wikipedia)" href="http://en.wikipedia.org/wiki/Cardinality_(SQL_statements)" target="_blank">“high cardinality”</a> column, which means it does not lend itself well to indexing. This is a problem because most queries on this kind of high frequency data are to fetch a subset by timestamp, and you do NOT want to make a table scan of a billion plus records to find a few minutes of data.</p>

<p>TSDB attempts to fix this problem by keeping rows in order and creating a <a title="Dense versus Sparse Indexes" href="http://www.cs.sfu.ca/CourseCentral/354/zaiane/material/notes/Chapter11/node5.html" target="_blank">sparse index</a> (an index on every 20,000 rows). This should work in theory, but you must ensure that your rows are always sequential. That makes inserting or updating difficult, because you potentially need to shift rows to maintain the order. Also, I’m not aware of a relational database that lets you create a sparse index, which rules out the most common and best understood data stores.</p>

<p>Another approach is to <em>partition</em> your data. This is my current approach. The way this works is you simply create multiple tables for set time periods (one table per day or month is a good starting point). You then put records that match those time periods in their respective tables and write some logic to union queries across tables.</p>

<p>Partitioning enables the database to hunt through a subset of rows to find the ones that match your query. Both Postgres and MySQL support partitioning, making them viable options for storing time series data. The library that I’m working on will use <a title="PyTables" href="http://pytables.github.io" target="_blank">PyTables</a> to partition time series data by date.</p>
<h2>Store timestamps in UTC</h2>
<p>Most of the time, your source data will have timestamps in UTC. If it doesn’t, I suggest you convert to UTC before storing. Most libraries use either UTC or local time internally, and because you can never be sure what time zone your users will be in, using UTC is the least common denominator.</p>

<p>UTC also has the nice property of not having daylight saving time changes. DST causes all sorts of pain when working with 24/7 data. Avoid it by just dealing in UTC internally, and then converting to other timezones for querying or display.</p>
<h2>Store timestamps as integers, even if your programming language uses floats</h2>
<p>MATLAB, Excel, and R all store timestamps internally as floats by default. This gives their timestamp types a large range and high precision, but I don’t think it is appropriate for archiving time series data. Why? Floats are imprecise. You do not know with any accuracy the number of significant digits when using a float, and you cannot make comparisons without worrying about round off errors. Admittedly, even with microsecond data, these systems that use a 64-bit double and 1970-01-01 as the epoch will not loose precision until <span style="color: #000000;">2242-03-16, but why worry about it? I recommend a 64-bit integer as the timestamp column. With one tick equaling one millisecond, you have a time range of <span style="color: #252525;">±</span><span style="color: #252525;">292 million years. This is Java’s internal representation. With one tick equaling 100 nanoseconds (0.1 microsecond), you have a time range of ±29,227 years, which is what Win32 does. Should be plenty!</span></span></p>
<h2>Have a solid ETL procedure</h2>
<p>ETL means “extract, transform, load” and is the term for taking data out of one format or system and loading it into another. The step to make sure is solid when you are dealing with high frequency data is the “load” step. Try to make a process where you can revert a botched load automatically. If you don’t do this, guaranteed someone or something will screw up an import, and you will be left wading through millions of rows of data to fix it or re-importing everything from your source system.</p>

<p>The most basic way to make the load step revertible is to just make a copy of your time series before writing anything to it. You could devise a more sophisticated process, like using rsync to make diffs of your time series, but be nice to your future self and make backups at the very least!</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2014/04/the-business-model-sketch">
        The Business Model Sketch
      </a>
    </h1>

    <span class="post-date">22 Apr 2014</span>

    <p>I was helping a friend work through some business ideas, and realized that like writing an outline helps you structure a essay, doing a “business model sketch” can help you break apart a business idea and evaluate its viability. Just like an essay needs a thesis, a body and a conclusion or it can fall flat, a business model needs specific components or it is not viable. I identified six components that roughly draw from <a title="The Personal MBA by Joshua Kaufman at Amazon" href="http://www.amazon.com/The-Personal-MBA-Master-Business/dp/1591845572" target="_blank">The Personal MBA</a> and the “lean startup” philosophy. If you are thinking of starting a new business, consider taking one page of paper, drawing six boxes, and filling out these six areas: customer, value proposition, marketing, sales, value delivery, and finance.</p>

<p>The problem with many business ideas is that, while they sometimes hit on a few of these areas, they are weak in others. That can be okay, but as a startup, your goal should be to develop a plan about how to address your idea’s weak points, and quickly test whether that plan is viable. If it is, great! If not, reconsider or proceed with caution!</p>
<h2>The template</h2>
<h3>The customer</h3>
<p>The specific person to whom value is being provided <strong>and is making a purchasing decision</strong>. Many times a business’s value proposition will include many people. For example, if you are selling family vacations, your business is offering value to the father (maybe you bundle in a few rounds of golf), mother (there’s a reason cruises have spas), and kids (I don’t think Disney pays actors to dress as Micky Mouse for dad). This is important to understand, especially when you get to the value delivery portion of the business model sketch. However, first identify the decision maker!</p>
<h3>Value proposition</h3>
<p>The value proposition is a description of the value you are providing your customer. Remember that your customer is the person making the purchasing decision, so when crafting your value proposition, you need to understand the wants and needs of that particular person above any secondary party.</p>

<p>Ideally, focus your value proposition on addressing core human drives. The Personal MBA identifies <a href="http://book.personalmba.com/core-human-drives/" target="_blank">five core human drives</a>: the drives to acquire, bond, learn, defend, and feel. Many consumer products clearly target their value propositions to these core human drives. This can be difficult if you are not selling consumer products or services, but if you look closely, many business-to-business or business-to-government sales are targeted the same way. Living in Washington, DC I notice this every time I go through the Pentagon metro station. Large defense contractors clearly target their buyer’s core human drive to defend with advertisements that depict the strength and sophistication of their offerings. This example of <a href="http://www.huffingtonpost.com/2013/09/06/drone-ads-dc-metro_n_3880026.html" target="_blank">advertisements for Northrop Grumman’s unmanned drones</a> is a great example.</p>
<h3>Marketing</h3>
<p>Marketing is fundamentally your strategy for getting your customer’s attention to deliver your value proposition. When you create a business model sketch, you need to attempt to identify how you will reach enough customers at an economical cost. If you have an excellent value proposition but your customers do not know about it, your business model will fail.</p>

<p>In the marketing portion of your sketch, you should also attempt to identify in rough terms the market size and dynamics: how many potential customers do you have and is that number increasing and/or gaining purchasing power?</p>
<h3>Sales</h3>
<p>If marketing is your strategy for reaching your target customer, sales is your strategy for negotiating a contract to deliver your value proposition in exchange for money. For an online business, this could be as simple as “sign up for PayPal and put a Buy Now button on my website”. If you are an enterprise software, this could be extensive contract negotiations.</p>

<p>Sales and marketing are closely intertwined, but separating them out to different boxes in the business model sketch should help you to separate the <em>act of reaching your customer and delivering your value proposition</em> (marketing) and <em>the mechanics of “closing the deal”</em> (sales).</p>
<h3>Value delivery</h3>
<p>This is what many people think of as the meat of your business model, but the previous sections are also key to determining its viability. Value delivery is the processes that delivery the promised value to your customers. In the family vacation example, this is the operations of your hotel from hiring staff to procuring food and drinks for the restaurant. If you are an online business, this is the cost of developing your website as well as operational cost of running your servers and supporting your customers.</p>
<h3>Finance</h3>
<p>The finance section should answer “what financial resources do I need to support this business model, and what is the return on investment?”. This is hard to answer in specifics in a business model sketch, and should be tackled in more detail in a complete business plan. However, once you complete the other sections, you should be able to answer these questions:</p>
<ul>
	<li>Is this a financial capital-intensive business to set up? To run? If yes, what is the risk to capital committed to the business? What return can I offer owners of capital given that risk? How can I source that capital, loans or equity investment?</li>
	<li>Is this a human capital-intensive business to set up? If yes, what is my recruitment strategy? Can I attract the right talent? Would I compensate them with equity or a salary?</li>
</ul>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2014/04/writing-zero-downtime-migrations-for-rails-and-postgres">
        Writing Zero-Downtime Migrations for Rails and Postgres
      </a>
    </h1>

    <span class="post-date">18 Apr 2014</span>

    <p>Let’s suppose you are building an app. It is under heavy development and the dev team is cranking out new features left and right. Developers need to continually change the database schema, but you don’t want to take down the app for database migrations if at all possible. How the heck do you do this with Rails?</p>

<p>We had this problem recently, and have come up with a procedure that solves it for most small database migrations. The goals of this procedure are to:</p>
<ul>
	<li>Avoid downtime by running database migrations while the app is live</li>
	<li>Avoid too many separate deployments to production</li>
	<li>Keep the application code as clean as possible</li>
	<li>Balance the cost of additional coding with the benefit of having a zero-downtime migration. If the cost or complexity of coding a migration in this way is too great, then a maintenance window is scheduled and the migration is written in a non-zero downtime fashion.</li>
</ul>
<p>The first thing to understand when writing a zero downtime migration is what types of Postgres data definition language (DDL) queries can be run without locking tables. As of Postgres 9.3, the following DDL queries can be run without locking a table:</p>
<pre class="toolbar:2 nums:false nums-toggle:false lang:pgsql decode:true">CREATE INDEX CONCURRENTLY</pre>
<p style="padding-left: 30px;">Postgres can create indexes concurrently (without table locks) in most cases. <code>CREATE INDEX CONCURRENTLY</code> can take significantly longer than <code>CREATE INDEX</code>, but it will allow both reads and writes while the index is being generated.</p>

<pre class="toolbar:2 nums:false nums-toggle:false lang:pgsql decode:true">ALTER TABLE ... ADD COLUMN -- certain cases only!</pre>
<p style="padding-left: 30px;">You can add a column to a table without a table lock if the column being added is nullable and has no default value or other constraints.</p>
<p style="padding-left: 30px;">If you want to add a column with a constraint or a column with a default value, one option may be to add the column first without a default value and no constraint, then in a separate transaction set the default value (using <code>UPDATE</code>)  or use <code>CREATE INDEX CONCURRENTLY</code> to add a index that will be used for the constraint. Finally, a third transaction can add the constraint or default to the table. If the third transaction is adding a constraint that uses an existing index, no table scan is required.</p>

<pre class="toolbar:2 nums:false nums-toggle:false lang:pgsql decode:true">ALTER TABLE ... DROP COLUMN</pre>
<p style="padding-left: 30px;">Dropping a column only results in a metadata change, so it is non-blocking. When the table is <code>VACUUMED</code>, the data is actually removed.</p>

<pre class="toolbar:2 nums:false nums-toggle:false lang:pgsql decode:true">CREATE TABLE, CREATE FUNCTION</pre>
<p style="padding-left: 30px;">Creating a table or a function is obviously safe because no one will have a lock on these objects before they are created.</p>

<h2>Process for Coding the Migration</h2>
<p>The guidelines I have been using for writing a zero-downtime migration are to:</p>
<ul>
	<li><strong>Step 1: </strong>Write the database migration in Rails.</li>
	<li><strong>Step 2: </strong>Modify the application code in such a way that it will work both before and after the migration has been applied (more details on this below). This will probably entail writing code that branches depending on that database state.</li>
	<li><strong>Step 3: </strong>Run your test suite with the modified code in step 2<i> but before you apply the database migration!</i></li>
	<li><strong>Step 4: </strong>Run your test suite with the modified code in step 2 <em>after applying the database migration</em>. Tests should pass in both cases.</li>
	<li><strong>Step 5: </strong>Create a pull request on Github (or the equivalent in whatever tool you are using). Tag this in such a way that whoever is reviewing your code knows that there is a database migration that needs careful review.</li>
	<li><strong>Step 6: </strong>Create a separate pull request on Github that cleans up the branching code you wrote in step 2. The code you write in this step can assume that the DB is migrated.</li>
</ul>
<p>When the migration is deployed, you’ll deploy first the code reviewed in step 5. This code will be running against the non-migrated database, but that is a-ok because you have tested that case in step 3. Next, you will run the migration “live”. Once the migration is applied, you will still be running the code reviewed in step 5, but against the migrated database. Again, this is fine because you have tested that in step 4.<strong>
</strong></p>

<p>Finally, once the production database has been migrated, you should merge your pull request from step 6. This eliminates the dead code supporting the unmigrated version of the database. You should write the code for step 6 at the same time you write the rest of this code. Then just leave the pull request open until you are ready to merge. The advantage of this is that you will be “cleaning up” the extraneous code while it is still fresh in your mind.</p>
<h2>Branching Application Code to Support Multiple DB States</h2>
<p>The key to making this strategy work is that you’ll need to write you application code in step 2 in a way that supports two database states: the pre-migrated state and the post-migrated state. The way to do this is to check the database state in the models and branch accordingly.</p>

<p>Suppose you are dropping a column called “deleted”. Prior to dropping the column, you have a default scope that excludes deleted rows. After dropping the column, you want the default scope to include all rows.</p>

<p>You would code a migration to do that like this:</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="k">class</span> <span class="nc">DropDeletedFromPosts</span> <span class="o">&lt;</span> <span class="no">ActiveRecord</span><span class="o">::</span><span class="no">Migration</span>
   <span class="k">def</span> <span class="nf">up</span>
     <span class="n">drop_column</span> <span class="ss">:posts</span><span class="p">,</span> <span class="ss">:deleted</span>
   <span class="k">end</span>

   <span class="k">def</span> <span class="nf">down</span>
      <span class="n">add_column</span> <span class="ss">:posts</span><span class="p">,</span> <span class="ss">:boolean</span><span class="p">,</span> <span class="ss">:deleted</span><span class="p">,</span> <span class="ss">default: </span><span class="kp">false</span><span class="p">,</span> <span class="ss">null: </span><span class="kp">false</span>
   <span class="k">end</span>
<span class="k">end</span>
</code></pre>
</div>

<p>Then, in your Post model, you’d add branching like this:</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Post</span> <span class="o">&lt;</span> <span class="no">ActiveRecord</span><span class="o">::</span><span class="no">Base</span>

   <span class="c1"># TODO: Remove this code after the DropDeletedFromPosts migration has</span>
   <span class="c1"># been applied.</span>
   <span class="k">if</span> <span class="no">Post</span><span class="p">.</span><span class="nf">attribute_names</span><span class="p">.</span><span class="nf">include?</span> <span class="s1">'deleted'</span>
      <span class="n">default_scope</span> <span class="o">-&gt;</span> <span class="p">{</span> <span class="n">where</span><span class="p">(</span><span class="ss">deleted: </span><span class="kp">false</span><span class="p">)</span> <span class="p">}</span>
   <span class="k">end</span>

   <span class="c1"># Other model code here ...</span>

<span class="k">end</span>
</code></pre>
</div>

<h3> But doesn't this get complicated for larger migrations?</h3>
<p>Yes, absolutely it does. What we do when branching like this and it gets too complicated, we either sequence the DB changes over multiple deployments (and multiple sprints in the Agile sense) or “give up” and schedule a maintenance window (downtime) to do the change.</p>

<p>Writing zero-downtime migrations is not easy, and you’ll need to do a cost-benefit analysis between scheduling downtime and writing lots of hairy branching code to support a zero-downtime deploy. That decision will depend on how downtime impacts your customers and your development schedule.</p>

<p>Hopefully, if you decide to go the zero-downtime route, this procedure will make your life easier!</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2014/03/querying-inside-postgres-json-arrays">
        Querying Inside Postgres JSON Arrays
      </a>
    </h1>

    <span class="post-date">14 Mar 2014</span>

    <p>Postgres JSON support is pretty amazing. I’ve been using it extensively for storing semi-structured data for a project and it has been great for that use case. In Postgres 9.3, the maintainers added the ability to perform some simple queries on JSON structures and a few functions to convert from JSON to Postgres arrays and result sets.</p>

<p>One feature that I couldn’t figure out how to implement using the built-in Postgres functions was the ability to query within a JSON array. This is fairly critical for lots of the reporting queries that I’ve been building over the part few days. Suppose you have some JSON like this, stored in two rows in a table called “orders”, in the column “json_field”:</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="c1">// Row 1, "json_field" column -----</span>
<span class="p">{</span>
   <span class="s2">"products"</span><span class="err">:</span> <span class="p">[</span>
      <span class="p">{</span> <span class="s2">"id"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">"name"</span><span class="p">:</span> <span class="s2">"Fish Tank"</span> <span class="p">},</span>
      <span class="p">{</span> <span class="s2">"id"</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">"name"</span><span class="p">:</span> <span class="s2">"Bird Feeder"</span> <span class="p">}</span>
   <span class="p">]</span>
<span class="p">}</span>

<span class="c1">// Row 2, "json_field" column -----</span>
<span class="p">{</span>
   <span class="s2">"products"</span><span class="err">:</span> <span class="p">[</span>
      <span class="p">{</span> <span class="s2">"id"</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s2">"name"</span><span class="p">:</span> <span class="s2">"Bird Feeder"</span> <span class="p">},</span>
      <span class="p">{</span> <span class="s2">"id"</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span> <span class="s2">"name"</span><span class="p">:</span> <span class="s2">"Cat Pole"</span> <span class="p">}</span>
   <span class="p">]</span>
<span class="p">}</span>
</code></pre>
</div>

<p>If you want to run a query like “find all distinct IDs in the json_field’s products array”, you can’t do that with the built in JSON functions that Postgres currently supplies (as far as I’m aware!). This is a fairly common use case, especially for reporting.</p>

<p>To get this work, I wrote this simple PgPL/SQL function to map a JSON array.</p>

<pre><code class="language-pgpsql">CREATE OR REPLACE FUNCTION json_array_map(json_arr json, path TEXT[]) RETURNS json[]
LANGUAGE plpgsql IMMUTABLE AS $$
DECLARE
	rec json;
	len int;
	ret json[];
BEGIN
	-- If json_arr is not an array, return an empty array as the result
	BEGIN
		len := json_array_length(json_arr);
	EXCEPTION
		WHEN OTHERS THEN
			RETURN ret;
	END;

	-- Apply mapping in a loop
	FOR rec IN SELECT json_array_elements#&gt;path FROM json_array_elements(json_arr)
	LOOP
		ret := array_append(ret,rec);
	END LOOP;
	RETURN ret;
END $$;
</code></pre>

<p>What this function does is given a JSON array as “json_arr” and a JSON path as “path”, it will loop through all elements of the JSON array, locate the element at the path, and store it in a Postgres native array of JSON elements. You can then use other Postgres array functions to aggregate it.</p>

<p>For the query above where we want to find distinct product IDs in the orders table, we could write something like this:</p>

<pre><code class="language-pgpsql">SELECT DISTINCT unnest(json_array_map(orders.json_field#&gt;'{products}', '{id}'::text[]))::text AS "id" FROM orders;
</code></pre>

<p>That would give you the result:</p>
<pre> id
-----
 2
 3
 1</pre>
<p>Pretty cool!</p>

  </div>
  
  <div class="post">
    <h1 class="post-title">
      <a href="/2014/02/volatility-of-bitcoin-index">
        Volatility of Bitcoin Index
      </a>
    </h1>

    <span class="post-date">24 Feb 2014</span>

    <p>A while back when I was a research assistant at the Federal Reserve, I worked on a project to make exchange rate volatility indexes for the major world currencies. We basically had some high frequency data for USD/EUR, USD/CHF, and USD/JPY and wanted to see how the financial crisis affected volatility. With all of the hype and turmoil around Bitcoin, I though it would be interesting to make a similar index for the Bitcoin/USD exchange rate.</p>

<p>Before Bitcoin is ever able to become a viable “currency”, volatility needs to come down a lot. Low volatility isn’t sufficient for it to take off, but is probably is necessary. If you take the traditional definition of a currency as a “store of value”, a “medium of exchange”, and a “unit of account”, persistently high volatility is absolutely a death knell. This is especially true in Bitcoin’s case where there is no government backing and there are attractive substitutes in traditional currencies.</p>

<p>One of the cool things about Bitcoin, however, is that lots of the data is fairly open. Most of the rest of the financial market data in the U.S. is behind the copyright lock and seal of the major exchanges and electronic trading networks. Both the NYSE and NASDAQ make lots of money off of selling market data, and they recently won <a title="SEC court case on market data fees" href="http://www.reuters.com/article/2013/04/30/us-sec-exchanges-netcoalition-idUSBRE93T0Q220130430">a court case to raise their rates even further</a>. That makes doing this kind of analysis on other securities an expensive endeavor for armchair quarterbacks like myself!</p>

<p>Bitcoin, however, has a rather open ethos and most of the exchanges publish realtime or near-realtime price data for free. CoinDesk aggregates this into their <a title="Bitcoin Price Index" href="http://www.coindesk.com/price/">Bitcoin Price Index</a>, which they graciously agreed to send over for this analysis.</p>

<p>The Volatility of Bitcoin Index (VOB Index—I’m open to suggestions on the name) is a simple rolling standard deviation of minutely log-returns. That is not nearly as complicated as it sounds. To create the index, I started with a time series of minutely Bitcoin price data and calculated the <a title="Wikipedia - Log Returns" href="http://en.wikipedia.org/wiki/Rate_of_return#Comparing_ordinary_return_with_logarithmic_return">log returns</a> (basically percent increase or decrease for each minute).  Then for each period, I took all of the returns within a trailing window and computed the standard deviation. I did this for three window lengths: 30, 60 and 90 days, and then annualized it. The Bitcoin markets are 24/7/365, so there are no holiday or weekend adjustments, which makes things a bit easier.</p>

<p>Here’s what the index looks like for the period August 2011 to January 31, 2014.</p>

<p><img class="aligncenter size-full wp-image-256" alt="Volatility of Bitcoin Index" src="http://andyfiedler.com/wp-content/uploads/2014/02/vob_plot.png" width="800" height="462" />Here’s the BPI (Bitcoin Price Index) over that period.</p>

<p><img class="aligncenter size-full wp-image-257" alt="BPI Index" src="http://andyfiedler.com/wp-content/uploads/2014/02/bpi_plot.png" width="800" height="462" />
Bitcoin volatility looks like it is dropping over time, especially from the early days in 2011 and 2012, except for a large bump in volatility in April of 2013. That was probably sparked by problems at Bitcoin’s largest exchange, Mt. Gox during that time period. This decrease is despite an increase in speculative interest in Bitcoin and moves by China to curtail the currency’s use.</p>

<p>Whether Bitcoin takes off is anyone’s guess and predicting if it does is probably a fool’s errand. There are lots of smart people working on it, but big questions about its viability still remain. In either case, it should be exciting to watch!</p>

  </div>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/page11">Older</a>
  
  
    
      <a class="pagination-item newer" href="/page9">Newer</a>
    
  
</div>
    </div>

    
  </body>
</html>
